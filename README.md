# Projet final : *InsightPipeline*
## Irina STERLIGOVA & David LECONTE - POEI Jems/LePont

Spark ETL pipeline

L'objectif principal de ce projet est de fournir un pipeline spark qui soit flexible et qui puisse √™tre modifi√© en fonction des diff√©rentes options de traitement des donn√©es. L'accent est mis sur la simplicit√© et la logique du pipeline et de ses configurations, ce qui permet de l'adapter facilement en fonction des besoins de l'utilisateur. 

## Architecture du code et √©tapes cl√©s

Pour faciliter l'utilisation du code et l'ajout rapide des modifications n√©cessaires √† chaque projet, chaque √©tape du pipeline a √©t√© plac√©e dans un fichier Python ind√©pendant. 

```
src\insight_pipeline_package
 --ingest.py (fonction de lecture des donn√©es)
 --load.py (fonction de t√©l√©chargement des donn√©es )
 --pipeline_congig.py (une classe qui d√©finit les param√®tres de configuration: spark session name, raw data file location, path to operational data store (ods) layer, path to data mart layer, column name for data aggregation sum etc.
  
 --spark_pipeline.py (principale fonction ex√©cutive de la pipeline)
 --transform.py (fonctions de traitement et d'agr√©gation des donn√©es)
 --utils.py (D√©marrer et arr√™ter une session spark)

```
Les √©tapes cl√©s: 

- lancement de la session spark
- lecture des donn√©es brutes
- traitement et agr√©gation des donn√©es
- t√©l√©chargement des donn√©es vers la couche n√©cessaire


### Documentation

Les dossiers **src** et **tests** contiennent respectivement une documentation du package et de la suite de tests.

### Stockage des donn√©es

Les donn√©es finales seront stock√©es au format CSV.

Le dossier **data** est structur√© de mani√®re √† ce qu'√† l'int√©rieur, le dossier **raw** puisse stocker les .csv bruts, le dossier **ods** (operational data store) puisse stocker les .csv nettoy√©s, et le dossier **dml** (data mart layer) puisse stocker les vues m√©tiers .csv.

```
data
 --raw (bronze data)
 --ods (Operational data store, silver data)
 --dml (Data mart layer, gold data)
```

### Choix du langage

Nous avons √©tabli qu'il √©tait suffisant, plus simple et par cons√©quent plus efficace d'utiliser *Python* pour ce projet.

### CI/CD

Dans notre suite de tests, nous v√©rifions √† chaque fois  √† chaque fois si les dataframes cr√©√©s contiennent des donn√©es sans doublons et que toutes les lignes contiennent des valeurs.

Un workflow Github Actions v√©rifie si les tests sont valid√©s correctement √† chaque op√©ration *push*.

√Ä chaque push sur la branche *main*, un workflow Github Actions publie le package .whl contenant la pipeline.

üì¶ Pypi: https://test.pypi.org/project/insight-pipeline/


## Dev section

### Build package

```bash
python3 -m build
```

### Run in local

La version de d√©mo a √©t√© utilis√©e pour tester localement l'op√©rabilit√© du code.
Note, you need to install `spark`, `java 17` and `hadoop` locally

```bash
python3 ./demo.py 
```

### Run tests

```bash
python -m pytest tests/
```

### Publish package

1. Update package version in `pyproject.toml`
2. Create git tag

```bash
git tag 0.0.x
```

3. Push tag to github

```bash
git push origin --tags
```